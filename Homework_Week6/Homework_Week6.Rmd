---
title: "Homework_Week6"
author: "Meaghan Burke"
date: "October 12, 2019"
output: html_document
fig_width: 12 
fig_height: 4 
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(fma)
library(fpp2)
library(gridExtra)
library(urca)
```


### Week 6 Assignment 


1. Figure 8.31 shows the ACFs for 36 random numbers, 360 random numbers and 1,000 random numbers.

*Figure 8.31: Left: ACF for a white noise series of 36 numbers. Middle: ACF for a white noise series of 360 numbers. Right: ACF for a white noise series of 1,000 numbers*
  
  a. Explain the differences among these figures. Do they all indicate that the data are white noise?
  
  - Autocorrelation is the measurement of how correlated some lag in the time series is to the current lag. For ACF a previous point in time can either be directly or indirectly impacting the current value. For all three series objects, the autocorrelations, displayed with the ACF plots, are within the 95% error terms (essentially no different than zero). This confirms that all three series objects are essentially random and fit the definition of "White Noise" or ARIMA(0, 0, 0). The main difference between all three series objects is the number of samples. The smaller the number samples, the larger the ACF bars.


  b. Why are the critical values at different distances from the mean of zero? Why are the autocorrelations different in each figure when they each refer to white noise?
  
  - The error terms are a function of the number of samples in a series, $\frac{\frac{+}{-}1.96}{\sqrt{N}}$. The larger the number of samples, N, the closer the significance level/error terms are to zero. The error terms are larger for smaller data set, meaning higher series autocorrelation is needed to reject the null hypothesis that the series autocorrelation is zero. 
  
  

2. A classic example of a non-stationary series is the daily closing IBM stock price series (data set **ibmclose**). Use R to plot the daily closing prices for IBM stock and the ACF and PACF. Explain how each plot shows that the series is non-stationary and should be differenced.

```{r fig.width = 12, fig.height= 10}
ibm.autoplot<- autoplot(ibmclose) + ggtitle("CLOSING PRICE")
ibm.acf <- ggAcf(ibmclose) + ggtitle("ACF PLOT")
ibm.Pacf <- ggPacf(ibmclose) + ggtitle("PACF PLOT")
grid.arrange(
  ibm.autoplot,
  ibm.acf,
  ibm.Pacf,
  nrow = 2,
  top = "IBM US EQUITY")
```

  - **IBM Closing Price Plot** Non-stationary data has a mean that is either increasing or decreasing with time. Stationary data has a constant mean/variance the whole series. It does not matter when observed, it should look much the same at any point in time. No predictable patterns in the long-term. Time series plots will be roughly horizonta. In the IBM US EQUITY closing price plot, there is clearly a decreasing trend over time, indicating that this time series needs to be differenced.
  
  
  - **IBM Closing Price ACF Plot** ACF plots are great for testing of stationarity. For the IBM trended data, the lags are well outside the 95% "error terms" confidence interval meaning that they are meaningfully and serially  correlated. The IBM ACF decreases slowly and its ACF lags are large and positive
  
  
  - **IBM Closing Price PACF Plot** PACF plots are also great for testing of stationarity. For the IBM trended data, the first lag should be equal to the ACF first lag. The first PACF lag is close to one and all other PACF lags are close to zero, suggesting that the data is non-stationary.
 

6. Use R to simulate and plot some data from simple ARIMA models.

  a. Use the following R code to generate data from an AR(1) model with $\phi_{1}\ = 0.6$ and $\sigma^{2}\ = 1$ and $y_{1} =0$

```{r}
#created simulated.ar function from the given code so it can be reused in later components of the exercise 
simulated.ar <- function(N,phi, sd ){
  y <- ts(numeric(N))
  e <- rnorm(N, sd = sd)
  
  for( i in 2:N){
   y[i] <- phi*y[i-1] + e[i] 
    
  }
  return(y)
  
}

initial.ar <- simulated.ar(100, 0.6, 1)
autoplot(initial.ar,col="purple")+ ggtitle("AR(1) Model PLOT")
```

  b. Produce a time plot for the series. How does the plot change as you change  $\phi_{1}\$ ?
  
```{r}
initial.ar <- autoplot(simulated.ar(100, 0.6, 1),col="purple")+ ggtitle("AR(1) Model PLOT, PHI = 0.6")
second.ar <- autoplot(simulated.ar(100,  0.06, 1), col="blue")+ ggtitle("AR(1) Model PLOT, PHI = 0.06")
third.ar <- autoplot(simulated.ar(100,  0.006, 1),col="pink")+ ggtitle("AR(1) Model PLOT, PHI = 0.006")
fourth.ar <- autoplot(simulated.ar(100,  0.0006, 1),col="red")+ ggtitle("AR(1) Model PLOT, PHI = 0.0006")

grid.arrange(
  initial.ar,
  second.ar,
  third.ar,
  fourth.ar,
  nrow = 2,
  top = "AR(1) MODEL @ Different PHI Values")
```
  
  - Visually, it appears that with smaller $\phi_{1}\$, the more random the data. As per the above exercises, the auto correlation lags should be higher for the larger $\phi_{1}\$ values. The below ACF plots confirms the higher ACF plots.
  
```{r}
initial.ar.acf <- ggAcf(simulated.ar(100, 0.6, 1),col="purple")+ ggtitle("AR(1) Model ACF PLOT, PHI = 0.6")
second.ar.acf <- ggAcf(simulated.ar(100,  0.06, 1), col="blue")+ ggtitle("AR(1) Model ACF PLOT, PHI = 0.06")
third.ar.acf <- ggAcf(simulated.ar(100,  0.006, 1),col="pink")+ ggtitle("AR(1) Model ACF PLOT, PHI = 0.006")
fourth.ar.acf <- ggAcf(simulated.ar(100,  0.0006, 1),col="red")+ ggtitle("AR(1) Model ACF PLOT, PHI = 0.0006")

grid.arrange(
  initial.ar.acf,
  second.ar.acf,
  third.ar.acf,
  fourth.ar.acf,
  nrow = 2,
  top = "AR(1) MODEL ACF @ Different PHI Values")
```  
  
  c. Write your own code to generate data from an MA(1) model with$\theta_{1}\ = 0.6$ and $\sigma^{2}\ = 1$
  
```{r}
simulated.ma <- function(N,theta, sd ){
  y <- ts(numeric(N))
  e <- rnorm(N, sd = sd)
  e[1] <- 0 
  for( i in 2:N){
   y[i] <- theta*e[i-1] + e[i] 
    
  }
  return(y)
  
}  

initial.ma <- simulated.ma(100, 0.6, 1)
autoplot(initial.ma,col="purple")+ ggtitle("MA(1) Model PLOT")  
```

    d. Produce a time plot for the series. How does the plot change as you change  $\theta_{1}\$ ?
    
```{r}
initial.ma <- autoplot(simulated.ma(100, 0.6, 1),col="purple")+ ggtitle("MA(1) Model PLOT, THETA = 0.6")
second.ma <- autoplot(simulated.ma(100,  0.06, 1), col="blue")+ ggtitle("MA(1) Model PLOT, THETA = 0.06")
third.ma <- autoplot(simulated.ma(100,  0.006, 1),col="pink")+ ggtitle("MA(1) Model PLOT, THETA = 0.006")
fourth.ma <- autoplot(simulated.ma(100,  0.0006, 1),col="red")+ ggtitle("MA(1) Model PLOT, THETA = 0.0006")

grid.arrange(
  initial.ma,
  second.ma,
  third.ma,
  fourth.ma,
  nrow = 2,
  top = "MA(1) MODEL @ Different THETA Values")
```

- Similar to AR(1), the smaller $\theta_{1}\$ in the MA(1), the more random the data appears. Below are ACF plots of the MA(1) plots to confirm.

```{r}
initial.ma.acf <- ggAcf(simulated.ma(100, 0.6, 1),col="purple")+ ggtitle("MA(1) ACF PLOT, THETA = 0.6")
second.ma.acf <- ggAcf(simulated.ma(100,  0.06, 1), col="blue")+ ggtitle("MA(1) ACF PLOT, THETA = 0.06")
third.ma.acf <- ggAcf(simulated.ma(100,  0.006, 1),col="pink")+ ggtitle("MA(1) ACF PLOT, THETA = 0.006")
fourth.ma.acf <- ggAcf(simulated.ma(100,  0.0006, 1),col="red")+ ggtitle("MA(1) ACF PLOT, THETA = 0.0006")

grid.arrange(
  initial.ma.acf,
  second.ma.acf,
  third.ma.acf,
  fourth.ma.acf,
  nrow = 2,
  top = "MA(1) ACF PLOTS @ Different THETA Values")
```




  e. Generate data from an ARMA(1,1) model with $\phi_{1}\ = 0.6$,  $\theta_{1}\ = 0.6$ and  $\sigma^{2}\ = 1$
  
  
```{r}
simulated.arma <- function(N, phi, theta, sd ){
  y <- ts(numeric(N))
  e <- rnorm(N, sd = sd)
  
  for( i in 2:N){
   y[i] <- phi*y[i-1] + theta*e[i-1] +e[i] 
    
  }
  return(y)
  
}  

initial.arma <- simulated.arma(100, 0.6, 0.6, 1)
autoplot(initial.arma,col="purple")+ ggtitle("ARMA(1) Model PLOT")  
```  

  f. Generate data from an ARMA(2) model with $\phi_{1}\ = -0.8$,  $\phi_{2}\ = 0.3$ and  $\sigma^{2}\ =  1$ 
  
```{r}
simulated.arma.ns <- function(N, phi1, phi2, sd ){
  y <- ts(numeric(N))
  e <- rnorm(N, sd = sd)
  
  #2:N ran an error, changed to 3 and worked. Length zero error
  for( i in 3:N){
    y[i] <- phi1*y[i-1] + phi2*y[i-2] + e[i]
 
    
  }
  return(y)
  
}  

non.standard.arma <- simulated.arma.ns(100, -0.8, 0.3, 1)
autoplot(non.standard.arma,col="purple")+ ggtitle("ARMA(2) Model PLOT")  
```    

  g. Graph the latter two series and compare them.
  
    - The AR(2) series exhibits non-stationary traits with a sinusoidal pattern amptitude curve that increases exponentially over time. The AR(2) series appears to have seasonality. The ARMA(1) series does not have this apparent seasonality and it appears to be random and much more stationary than the AR(2) series.
    

7. Consider wmurders, the number of women murdered each year (per 100,000 standard population) in the United States.

  a. By studying appropriate graphs of the series in R, find an appropriate ARIMA(p,d,q) model for these data.
  
```{r fig.width = 12, fig.height= 10}
murder.autoplot<- autoplot(wmurders) + ggtitle("Women Murdered PLOT")
murder.acf <- ggAcf(wmurders) + ggtitle("ACF PLOT")
murder.Pacf <- ggPacf(wmurders) + ggtitle("PACF PLOT")
grid.arrange(
  murder.autoplot,
  murder.acf,
  murder.Pacf,
  nrow = 2,
  top = "Women Murdered")
```


    - Appears that the data does not exhibit a seasonal trend, but is not stationary as per the trend plot, ACF and PACF plots. Differencing is required, check how many levels are needed with ndiffs(). Re-plotted the EDA plots to confirm that the data is random. There is one significant spike in the first lag in the ACF and PACF plots. The appropriate ARIMA model appears to be ARIMA(1,2,1).
```{r}
#check the number of differencing needed
print(paste("Number of Differences (ndiffs()) ", ndiffs(wmurders)))
murder.diff <- wmurders %>% diff(1) %>% diff(1) #two levels as per ndiffs

murder.diff.autoplot<- autoplot(murder.diff) + ggtitle("Women Murdered PLOT")
murder.diff.acf <- ggAcf(murder.diff) + ggtitle("ACF PLOT")
murder.diff.Pacf <- ggPacf(murder.diff) + ggtitle("PACF PLOT")
grid.arrange(
  murder.diff.autoplot,
  murder.diff.acf,
  murder.diff.Pacf,
  nrow = 2,
  top = "Women Murdered- Differenced Data")

# Confirm stationarity with KPSS function
murder.diff %>% ur.kpss() %>% summary()
```

  b. Should you include a constant in the model? Explain.
  
  - The higher the value of d, the more rapidly the prediction intervals increase in size. The constant c has important effects on the long-term forecasts obtained from ARIMA models. For example, the d value deterimed to be appropriate is 2. at d =2, long-term forecasts with c =0  follow a straight line, with c!= 0, the long term forecasts follow a quadratic trend. Since d is large, having a non zero c could have drastic impact on forecast, so it will not be included.
  
  c. Write this model in terms of the backshift operator.
  - $(1-\phi_{1}B)(1-B)^{2}y_{t} = c + (1+\theta_{1}B )\epsilon_{t}$
  
  d. Fit the model using R and examine the residuals. Is the model satisfactory?
  
```{r}
#fit model
(murder.fit <- Arima(wmurders, order=c(1,2,1)))

#check residuals to make sure the model is satisfactory 

res <- autoplot(murder.fit$residuals)
acf.model <- ggAcf(ts(murder.fit$residuals))+ ggtitle("ACF Residuals")
pacf.model <- ggPacf(ts(murder.fit$residuals))+ggtitle("PACF Residuals")

grid.arrange(
  res,
  acf.model,
  pacf.model,
  nrow = 2,
  top = "Residuals Check- Is the model satisfactory?")
```
    - Residuals appear to show stationarity, confirming that the model is satisfactory. The residual plots appear to be random and the ACF & PACF indicate that the model is free of auto-correlation. 

  e. Forecast three times ahead. Check your forecasts by hand to make sure that you know how they have been calculated.
  
  **still need to do by hand**
  
```{r}
# set the confidence level to 95% and the years out 3 years (h)
murder.forecast <- forecast(murder.fit, level = c(95), h=3 )
murder.forecast

```
  
  f. Create a plot of the series with forecasts and prediction intervals for the next three periods shown.
  
```{r}
plot(murder.forecast)
```

  g. Does auto.arima() give the same model you have chosen? If not, which model do you think is better?
  
```{r}
(autoarima.murder <- auto.arima(wmurders, seasonal=F))
```
    - auto.arima provided the same model as the one manually calculated ARIMA(1,2,1)

