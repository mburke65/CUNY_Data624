---
title: 'Data 624: Week 3 Homework'
author: "Angrand, Burke, Deboch, Groysman, Karr"
date: "October 12, 2019"
output:
  word_document: default
  pdf_document: default
  html_document: default
---

### Week 3 Assignment 


**Chapter 3 KJ 1 and 2**

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(mlbench)
library(psych)
library(caret)
library(PerformanceAnalytics)
library(pander)
library(MASS)
library(Amelia)
library(tidyverse)
```

##### 3.1 The UC Irvine Machine Learning Repository contains a data set related to glass identification. The data consist of 214 glass samples labeled as one of seven class categories. There are nine predictors, including the refractive index and percentages of eight elements: Na, Mg, Al, Si, K, Ca, Ba, and Fe. The data can be accessed via:

```{r, eval=TRUE, fig.width=18, fig.height=6}
data(Glass)
describe(Glass)
str(Glass)
my_df <- data.frame(Glass[,1:9])

cor(my_df)
```

  - A data frame with 214 observation containing examples of the chemical analysis of 7 different types of glass. 


  **a.** Using visualizations explore the predictor variables to understand their distributions as well as the relationships between predictors.

```{r, eval=TRUE}
#histograms for each
#ZERO-INFLATED NEGATIVE BINOMIAL for Mg, Ba & Fe or is it a nuanced distribution
par(mfrow = c(3,3))
hist(Glass[,'RI'],breaks=50)
hist(Glass[,'Na'],breaks=50)
hist(Glass[,'Mg'],breaks=50)
hist(Glass[,'Al'],breaks=50)
hist(Glass[,'Si'],breaks=50)
hist(Glass[,'K'],breaks=50)
hist(Glass[,'Ca'],breaks=50)
hist(Glass[,'Ba'],breaks=50)
hist(Glass[,'Fe'],breaks=50)
```
 
  - There are a total of 214 glass samples taken with no instances of missing data for any of the predictor variables.  Based upon their histograms and skewness, the predictors RI, Na, Al, Si & Ca display either either a normal distribution pattern or a distribution that could be transformed into a normal distribution pattern i.e. division by sqrt(s).  The remaining predictor variables Mg, K, Ba & Fe display concentrations of 0 frequency.  

```{r, eval=TRUE}
my_df <- data.frame(Glass[,1:9])
chart.Correlation(my_df, histogram=TRUE, pch=19)
```
 
  - From correlation we can see that:
    - RI is significantly positively correlated with CA and negatively correlated with AL,Si,K.
    - Na is Significantly positively correlated with Ba and negatively correlated with Mg,Al,K,Ca,Fe.
    - Mg is significantly negatively correlated with Ca,Ba ,Al.
    - Al is significantly positively correlated with K,Ba and negatively correlated with Ca.
    - Si is weakly negatively correlated with K and Cal. 

  **b.** Do there appear to be any outliers in the data?  Are any predictors skewed?

  - From the above plot of histograms we can see that Mg,Si,K,Ca,Ba and Fe has outliers. Fe,Ba,Ca,K,Na,RI are positively skewed and Mg,Si are negatively skewed.

  **c.** Are there any relevant transformations of one or more predictors that might improve the classification model?

```{r, eval=TRUE}

Glass$Type <- as.numeric(Glass$Type)
par(mfrow = c(3,3))
boxcox(Type~RI, data = Glass)
boxcox(Type~Na, data = Glass)
boxcox(Type~Mg, data = Glass)
boxcox(Type~Al, data = Glass)
boxcox(Type~Si, data = Glass)
boxcox(Type~K, data = Glass)
boxcox(Type~Ca, data = Glass)
boxcox(Type~Ba, data = Glass)
boxcox(Type~Fe, data = Glass)
```

  - A better solution to handling the predictors with concentrations of 0 frequency is to use a zero-inflated binary distribution for continuous data. The two predictors with the greatest correlation are RI and Ca suggesting that in a multivariable regression model, one of these explanatory variables could be removed because it is strongly co-linear with the other thus having little to no loss of predictive ability to the model. Also, from the box cox transformation plot we can see that log transformation of Na, Mg and Ba will improve the model



##### 3.2 The soybean data can also be found at the UC Irvine Machine Learning Repository. Data were collected to predict disease in 683 soybeans. The 35 predictors are mostly categorical and include information on the environmental conditions (e.g., temperature, precipitation) and plant conditions (e.g., left spots, mold growth). The outcome labels consist of 19 distinct classes.

```{r, eval=TRUE, fig.width=18, fig.height=6}
#Preliminary EDA
#Data Access
data(Soybean)
#Sampling
glimpse(Soybean)
#Shape
dim(Soybean)
#Stats 
describe(Soybean)
```


  - There are 19 classes, only the first 15 of which have been used in prior work. There are 35 categorical attributes, some nominal and some ordered. The value "dna" means does not apply. The values for attributes are encoded numerically, with the first value encoded as "0," the second as "1," etc.

**a.** Investigate the frequency distributions for the categorical predictors.  Are any of the distributions degenrate in ways discussed earlier in this chapter?

```{r, eval=TRUE}
df <- Soybean[,2:36]
par(mfrow = c(3, 6))
for (i in 1:ncol(df)) {
  barplot(table(df[,i]),ylab = names(df[i]))
}
```

nearZeroVar in R for the categorical variables

```{r, eval=TRUE}
nearZeroVar(df,names = TRUE, saveMetrics=T)
```

- There are few distributions degenerate . Specifically leaf.mild,mycelium and sclerotia.

**b.** Roughly 18% of the data are missing.  Are there particular predictors that are more likely to be missing?  Is the pattern of missing data related to the classes?
```{r, eval=TRUE, fig.width=18, fig.height=10}

missmap(Soybean)
```
```{r, eval=TRUE}
sort(colMeans(is.na(Soybean)),decreasing = T)
```

  - Particularly  hail, sever,seed.tmt,lodging, germ,leaf.mild fruiting.bodies, fruit.spots,seed.discolor,shriveling, leaf.shread, seed,mold.growth,seed.size,leaf.halo,are more likelly to be missing.

```{r, eval=TRUE}
Soybean %>%
mutate(total = n()) %>% 
group_by(Class) %>%
mutate(Missing = n(), Proportion=Missing/total) %>%
dplyr::select(Class, Missing, Proportion) %>%
unique() %>% 
  arrange(-Proportion)
```
**c.** Develop a strategy for handling missing data, either by eliminating predictors or imputation.

  - Drop the rows having missing values. After dropping, 562 observations remain.
  
```{r, eval=TRUE}
Soybean_complete <- na.omit(Soybean)
head(Soybean_complete)
dim(Soybean_complete)
```

